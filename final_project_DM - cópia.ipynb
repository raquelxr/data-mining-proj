{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation for a Sports Facility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "[1. Objectives](#1st-bullet)<br>\n",
    "[2. Import Data/Libraries](#2nd-bullet)<br>  \n",
    "[3. Data Exploration](#3rd-bullet)<br>  \n",
    "[4. Data Visualization](#4th-bullet)<br> \n",
    "[5. Pre-Processing](#5th-bullet)<br> \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<a class=\"anchor\" id=\"1st-bullet\">    </a>\n",
    "## 1. Objectives \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explore the data and identify the variables that should be used to segment customers.\n",
    "2. Identify customer segments\n",
    "3. Justify the number of clusters chosen (taking in consideration the business use as well).\n",
    "4. Explain the clusters found.\n",
    "5. Suggest business applications for the findings and define general marketing approaches for each cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<a class=\"anchor\" id=\"2nd-bullet\">    </a>\n",
    "## 2. Import Libraries/Data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil #round number to closest integer\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.stats import skew\n",
    "\n",
    "import sompy\n",
    "from sompy.visualization.mapview import View2D\n",
    "from sompy.visualization.bmuhits import BmuHitsView\n",
    "from sompy.visualization.hitmap import HitMapView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('XYZ_sports_dataset.csv', sep =';') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<a class=\"anchor\" id=\"3rd-bullet\">    </a>\n",
    "## 3. Data Exploration \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easily noted that our data contains missing values encoded as NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Date objects could be turned into a datetime type for easier manipulation and interpretation.\n",
    "- Binary variables are all int/float types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data['UseByTime'] = data['UseByTime'].astype(\"boolean\")\n",
    "data['AthleticsActivities'] = data['AthleticsActivities'].astype(\"boolean\")\n",
    "data['WaterActivities'] = data['WaterActivities'].astype(\"boolean\")\n",
    "data['FitnessActivities'] = data['FitnessActivities'].astype(\"boolean\")\n",
    "data['DanceActivities'] = data['DanceActivities'].astype(\"boolean\")\n",
    "data['TeamActivities'] = data['TeamActivities'].astype(\"boolean\")\n",
    "data['RacketActivities'] = data['RacketActivities'].astype(\"boolean\")\n",
    "data['CombatActivities'] = data['CombatActivities'].astype(\"boolean\")\n",
    "data['NatureActivities'] = data['NatureActivities'].astype(\"boolean\")\n",
    "data['SpecialActivities'] = data['SpecialActivities'].astype(\"boolean\")\n",
    "data['OtherActivities'] = data['OtherActivities'].astype(\"boolean\")\n",
    "data['HasReferences'] = data['HasReferences'].astype(\"boolean\")\n",
    "data['Dropout'] = data['Dropout'].astype(\"boolean\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()[data.isna().sum()!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some missing values that need to be adressed during the detailed exploration of each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicated clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=\"all\").T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the descriptive analysis, some possible problems appear:\n",
    "- For `Age` there seem to be some clients with age 0. The data is also skewed towards younger ages.\n",
    "- For `Income`, some clients have value 0. It also seems like some extreme values appear for high values of income.\n",
    "- The number of unique values shows that `LastPeriodStart` and `LastPeriodFinish` have some form of fixed dates.\n",
    "- `DaysWithoutFrequency`,`LifetimeValue`, `NumberOfFrequencies`, `AttendedClasses`, `AllowedNumberOfVisits` and `RealNumberOfVisits` also seem to have extreme high values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closer Look At Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ID'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All ID's are unique so we can set it as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('ID', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have one duplicated entry that needs to be removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(data['Age'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All age values seem normal for a sport facility.\n",
    "- Special attention to babies (0-3) and other childreen under 16 needs to be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Income:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Children under 16 should have no income:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Age']<16][data['Income']!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 360 entries that need to have income set as 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"Age\"] < 16 , \"Income\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Gender'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values for `Gender` are normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EnrollmentStart / EnrollmentFinish / LastPeriodStart / LastPeriodFinish / DateLastVisit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dates should be DateTime format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['EnrollmentStart'] = pd.to_datetime(data['EnrollmentStart'])\n",
    "data['EnrollmentFinish'] = pd.to_datetime(data['EnrollmentFinish'])\n",
    "\n",
    "data['LastPeriodStart'] = pd.to_datetime(data['LastPeriodStart'])\n",
    "data['LastPeriodFinish'] = pd.to_datetime(data['LastPeriodFinish'])\n",
    "\n",
    "data['DateLastVisit'] = pd.to_datetime(data['DateLastVisit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some entries have enrollment start equal to enrollment finish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['EnrollmentStart'] == data['EnrollmentFinish']]['Dropout'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All clients in this situation have 'Dropout' status set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['EnrollmentStart'] == data['EnrollmentFinish']]['LastPeriodFinish'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most cases seem to be of clients with current active contracts (Finishing on '2019-12-31'). \\\n",
    "We need to take a deeper look at other cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Members who have not been active in current Period (from 2019-07-01 untill 2019-12-31):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking clients who have not been to the facility in the current period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['DateLastVisit']< '2019-06-30', 'Dropout'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['DateLastVisit']< '2019-06-30'].loc[data['Dropout']== 0][data['EnrollmentFinish'] != data['EnrollmentStart']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The people who had DateLastVisit < '2019-06-30' and don't have a Dropout all have have 'EnrollmentStart'= 'EnrolmentFinish', which tells us that:\n",
    "- these people have dropped out but have not been added to the system as dropouts.\n",
    "- Their contract is still active as they've been paying, but not coming to the gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we deal with these inconsistencies?\n",
    "1) If `DateLastVisit` matches the Last Period of activity (not current Period), clients are considered dropouts (all date are correct but Enrollment Dates) and have `EnrollmentFinish` on `DateLastVisit`.\n",
    "2) Other clients are considered active: `LastPeriodFinish`should be '2019-12-31' and `EnrollmentFinish` on '2019-10-31'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropouts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a mask to select clients who follow condition **1** and should be considered dropouts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_mask = (\n",
    "    (data['EnrollmentStart'] == data['EnrollmentFinish']) &\n",
    "    (data['LastPeriodStart']<= data['DateLastVisit']) &\n",
    "    (data['DateLastVisit']<= data['LastPeriodFinish']) &\n",
    "    (data['DateLastVisit'] < pd.Timestamp(dt.date(2019,6,30)))\n",
    ")\n",
    "index_dropout = data.index[drop_mask].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dropout` should be set to 1 and `EnrollmentFinish` to the respective `DateLastVisit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[index_dropout, 'Dropout']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[index_dropout, 'EnrollmentFinish'] = data.loc[index_dropout, 'DateLastVisit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non - Drop Out:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the non - dropout clients, instances where `LastPeriodFinish` is not '2019-12-31' will be considered errors and removed, as they contain too many discrepencies to be considered viable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ( \n",
    "    (data['EnrollmentStart'] == data['EnrollmentFinish']) &\n",
    "    (data['DateLastVisit']< '2019-06-30') \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[mask]['LastPeriodFinish'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_mask = ( \n",
    "    (data['EnrollmentStart'] == data['EnrollmentFinish']) &\n",
    "    (data['DateLastVisit']< '2019-06-30') &\n",
    "    (data['LastPeriodFinish']!= pd.Timestamp(dt.date(2019,12,31)))\n",
    ")\n",
    "index_dropout = data.index[drop_mask].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(index_dropout, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other instances that have `EnrollmentStart` equal to `EnrollmentFinish` are considered active members:\\\n",
    "`LastPeriodFinish`should be '2019-12-31' and `EnrollmentFinish` on '2019-10-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[data['EnrollmentFinish'] == data['EnrollmentStart']].index.tolist(), 'LastPeriodFinish']='2019-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[data['EnrollmentFinish'] == data['EnrollmentStart']].index.tolist(), 'EnrollmentFinish']= '2019-10-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clients who have a LastPeriod that doesn't match neither the:\n",
    "- DateLastVisit: Last time client was at the facility\n",
    "- EnrollmentFinish: End of contract\n",
    "\n",
    "Are all dropouts that show inconsistencies in the date type variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    ~((data['LastPeriodStart'] <= data['EnrollmentFinish']) & (data['EnrollmentFinish'] <= data['LastPeriodFinish'])) &\n",
    "    ~((data['LastPeriodStart'] <= data['DateLastVisit']) & (data['DateLastVisit'] <= data['LastPeriodFinish']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.index[mask].tolist()].Dropout.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on clustering results these clients can either be:\n",
    "- Dropped totally from dataset as they represent 0.86% of entries;\n",
    "- Left as they are;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.drop(data.index[mask].tolist(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DaysWithoutFrequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading the metadata, `DaysWithoutFrequency` is said to be a variable that has values only for dropouts.\n",
    "\n",
    "Looking at our data, we see that this variable is calculated for all clients (even non dropouts), so we consider this variable with a new meaning:\\\n",
    "Days without frequency for all clients (until their current or last contract ended - `EnrollmentFinish`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Dropout'] == 0]['DaysWithoutFrequency'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have changed values for `EnrollmentFinish`, these values will have to be recalculated to express correct values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = ['AthleticsActivities', 'WaterActivities','FitnessActivities','DanceActivities','TeamActivities','RacketActivities','CombatActivities','NatureActivities','SpecialActivities','OtherActivities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fix some inconsistencies:\n",
    "- Until 3 years of age, clients can only do `WaterActivities`` or `OtherActivities``\n",
    "- Until 16 years of age, clients can't do fitness activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Children **under three** only have `WaterActivites` and 3 entries with `FitnessActivities` (that are droped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Age']<3][activities].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (data['Age']<3)\n",
    "    &(data['FitnessActivities'] ==1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.index[mask].tolist(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Children **under 16** have 12 entries with `FitnessActivities` (we turn these values into 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Age']<16]['FitnessActivities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "        (data['Age']<16)\n",
    "    &(data['FitnessActivities'] !=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.index[mask].tolist(), 'FitnessActivities'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AllowedNumberOfVisitsBySLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable represents the allowed number of visits, but is expressed in float form.\\\n",
    " Turning it into the closest integer should be done so the variable is coherent with it's meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AllowedNumberOfVisitsBySLA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HasReference / NumberOfReferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make sure these two variables are coherent with each other:\n",
    "- If clients has no refences, he can't have a number of references >0.\n",
    "- If client has references, he can't have a number of references = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (data['HasReferences']!=1)\n",
    "    &(data['NumberOfReferences']!=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.index[mask].tolist(), 'HasReferences'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some clients have `HasReferences` set to 1 but `NumberOfReferences` equal to 0.\\\n",
    "We change `HasReferences` to 0 so these entries become coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (data['HasReferences']==1)\n",
    "    &(data['NumberOfReferences']==0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.index[mask].tolist(), 'HasReferences'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LifetimeValue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['LifetimeValue'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The variables will be further explored using visualizations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<a class=\"anchor\" id=\"4th-bullet\">    </a>\n",
    "## 4. Data Visualization \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing our data can help us understand more about the distribution of the featues and find possible incoherences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by separating metric, non-metric and date type features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features = ['Age','Income','DaysWithoutFrequency','LifetimeValue','NumberOfFrequencies', 'AttendedClasses', 'AllowedWeeklyVisitsBySLA', 'AllowedNumberOfVisitsBySLA','RealNumberOfVisits','NumberOfRenewals','NumberOfReferences']\n",
    "date_features = ['EnrollmentStart','EnrollmentFinish','LastPeriodStart','LastPeriodFinish','DateLastVisit']\n",
    "non_metric_features = data.columns.drop(metric_features + date_features).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Numeric Variables' Histograms in one figure\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each histogram will be placed\n",
    "fig, axes = plt.subplots(2, ceil(len(metric_features) / 2), figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each histogram (hint: use the ax.hist() instead of plt.hist()):\n",
    "for ax, feat in zip(axes.flatten(), metric_features): # Notice the zip() function and flatten() method\n",
    "    ax.hist(data[feat], bins = 4)\n",
    "    ax.set_title(feat, y=-0.13)\n",
    "    \n",
    "# Layout\n",
    "# Add a centered title to the figure:\n",
    "title = \"Numeric Variables' Histograms\"\n",
    "\n",
    "plt.suptitle(title)\n",
    "\n",
    "\"\"\"\"\n",
    "To save pictures at the end\n",
    "if not os.path.exists(os.path.join('..', 'figures', 'exp_analysis')):\n",
    "    # if the exp_analysis directory is not present then create it first\n",
    "    os.makedirs(os.path.join('..', 'figures', 'exp_analysis'))\n",
    "    \n",
    "plt.savefig(os.path.join('..', 'figures', 'exp_analysis', 'numeric_variables_histograms.png'), dpi=200)\n",
    "\"\"\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Numeric Variables' Box Plots in one figure\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each box plot will be placed\n",
    "fig, axes = plt.subplots(2, ceil(len(metric_features) / 2), figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each box plot (hint: use the ax argument):\n",
    "for ax, feat in zip(axes.flatten(), metric_features): # Notice the zip() function and flatten() method\n",
    "    sns.boxplot(x=data[feat], ax=ax)\n",
    "    \n",
    "# Layout\n",
    "# Add a centered title to the figure:\n",
    "title = \"Numeric Variables' Box Plots\"\n",
    "\n",
    "plt.suptitle(title)\n",
    "\"\"\"\"\n",
    "# Save the figure\n",
    "if not os.path.exists(os.path.join('..', 'figures', 'exp_analysis')):\n",
    "    # if the exp_analysis directory is not present then create it first\n",
    "    os.makedirs(os.path.join('..', 'figures', 'exp_analysis'))\n",
    "    \n",
    "plt.savefig(os.path.join('..', 'figures', 'exp_analysis', 'numeric_variables_boxplots.png'), dpi=200)\n",
    "\"\"\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the above histograms and box plots, we notice that most variables have a skewed distribution that can symbolize the existence of outliers.\n",
    "- `NumberOfReferences` and `AttendedClasses` are the most skewed ones.\n",
    "\n",
    "This is extremely important information about our numeric variables, as these values can influence models negatively.\\\n",
    "Besides this behaviour, some other details were found:\n",
    "- `DaysWithoutFrequency` has people with more than 4 years without frequency;\n",
    "- `LifeTimeValue` seems to have clients with a total value of 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Relationship of Numerical Variables\n",
    "sns.set()\n",
    "\n",
    "# Setting pairplot\n",
    "sns.pairplot(data[metric_features], diag_kind=\"hist\")\n",
    "\n",
    "# Layout\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.suptitle(\"Pairwise Relationship of Numerical Variables\", fontsize=20)\n",
    "\"\"\"\"\n",
    "if not os.path.exists(os.path.join('..', 'figures', 'exp_analysis')):\n",
    "    # if the exp_analysis directory is not present then create it first\n",
    "    os.makedirs(os.path.join('..', 'figures', 'exp_analysis'))\n",
    "    \n",
    "plt.savefig(os.path.join('..', 'figures', 'exp_analysis', 'pairwise_relationship_of_numerical_variables.png'), dpi=200)\n",
    "\"\"\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the relationship plots above that:\n",
    "- `Age` and `Income` seem to have a positive relation;\n",
    "- `NumberOfFrequencies` and `AttendedClasses` seem to relate positively with `LifetimeValue`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Metric-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Non-Metric Variables' Absolute Frequencies\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each bar plot will be placed\n",
    "fig, axes = plt.subplots(2, ceil(len(non_metric_features) / 2), figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each bar plot (hint: use the ax argument):\n",
    "for ax, feat in zip(axes.flatten(), non_metric_features): # Notice the zip() function and flatten() method\n",
    "    sns.countplot(x=data[feat].astype(object), ax=ax, color='#007acc')\n",
    "\n",
    "title = \"Categorical Variables' Absolute Frequencies\"\n",
    "plt.suptitle(title)\n",
    "\n",
    "#plt.savefig(os.path.join('..', 'figures', 'exp_analysis', 'categorical_variables_frequecies.png'), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From observing the countplots, we notice that `DanceActivities` and `NatureActivities` have no entries set to 1.\\\n",
    "These variables can be considered uniformative and removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['NatureActivities','DanceActivities']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['NatureActivities','DanceActivities'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_metric_features.remove('NatureActivities')\n",
    "non_metric_features.remove('DanceActivities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities.remove('NatureActivities')\n",
    "activities.remove('DanceActivities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also look at the relation between age and certain activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=data.groupby(\"WaterActivities\")[\"Age\"].plot.hist(stacked=True,bins=60)\n",
    "plt.title('Water Activities By Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Absolute Frequency')\n",
    "plt.legend(title ='WaterActivities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=data, x='Age', hue='WaterActivities', bins = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=data, x='Age', hue='FitnessActivities', bins = 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<a class=\"anchor\" id=\"5th-bullet\">    </a>\n",
    "## 5. Pre-Processing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain why comes first**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is very skewed, so we're getting a very big number of outliers to remove using the standard methods and values, which is not acceptable consideting the problem in hand.\\\n",
    "To fight this problem, for the approaches used bellow, only very extreme outliers can be considered, which does not remove most detected outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine different methods to remove outliers, as to get more robust results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting outliers manually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distribution of our variables, we can select a threshold of values we consider extreme outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters0 = (\n",
    "    (data['NumberOfReferences']<2)\n",
    "    &(data['NumberOfRenewals']<6)\n",
    "    &(data['AttendedClasses']<400)\n",
    "    &(data['LifetimeValue']<4000)\n",
    "    &(data['RealNumberOfVisits']<60)\n",
    ")\n",
    "\n",
    "df_0 = data[filters0]\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_0.shape[0] / data_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IQR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q25 = data[metric_features].quantile(.25)\n",
    "q75 = data[metric_features].quantile(.75)\n",
    "iqr = (q75 - q25)\n",
    "\n",
    "upper_lim = q75 + 1.5 * iqr\n",
    "lower_lim = q25 - 1.5 * iqr\n",
    "\n",
    "filters1 = []\n",
    "for metric in metric_features:\n",
    "    llim = lower_lim[metric]\n",
    "    ulim = upper_lim[metric]\n",
    "    filters1.append(data[metric].between(llim, ulim, inclusive='both'))\n",
    "\n",
    "filters1 = pd.Series(np.all(filters1, 0))\n",
    "filters1.index = data.index\n",
    "df_1 = data[filters1]\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_1.shape[0] / data_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Z-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters2 = []\n",
    "for metric in metric_features:\n",
    "    mean, std = np.mean(data[metric]), np.std(data[metric])\n",
    "    z_score = np.abs((data[metric] - mean) / std)\n",
    "    filters2.append(z_score < 3) \n",
    "\n",
    "filters2 = pd.Series(np.all(filters2, 0))\n",
    "filters2.index = data.index\n",
    "df_2 = data[filters2]\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_2.shape[0] / data_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = data[(filters1 | filters2| filters0 )]\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_3.shape[0] / data_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our robust approach gives us the same results as applying the manual filter.\n",
    "\n",
    "Considering the skewness of our data, keeping this approach for an initial outlier removal is perfered, as we opt to remove a maximum of 1% of our data and both IQR and Z-Score tend to consider close to 5% of our data as very extreme outliers (making our threshold bigger converges to these values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values (Data Imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only after our outilers are removed can we work of data imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric Features- KNN Imputer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our numerical variables, we focus on using a KNNImputer to impute missing values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mv = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_metric_features = ['Income','NumberOfFrequencies','AllowedWeeklyVisitsBySLA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with a KNN model, we nedd to understand the optimal number of neighbours (K).\\\n",
    " To do so, we apply an **Elbow Method** considering we want to optimize the skewness of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_skewness = skew(data[mv_metric_features], axis=0, nan_policy='omit')\n",
    "\n",
    "print(original_skewness)\n",
    "k_values = range(1, 11)\n",
    "\n",
    "skewness_difference = []\n",
    "\n",
    "for k in k_values:\n",
    "    imputer = KNNImputer(n_neighbors=k, weights=\"distance\")\n",
    "    imputed_data = imputer.fit_transform(data_mv[mv_metric_features])\n",
    "    # so it ignores nans\n",
    "    imputed_skewness = skew(imputed_data, axis=0, nan_policy='omit')\n",
    "    \n",
    "    skewness_diff = np.mean(np.abs(original_skewness - imputed_skewness))\n",
    "    skewness_difference.append(skewness_diff)\n",
    "print(skewness_difference)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, skewness_difference, marker='o')\n",
    "plt.title('Skewness Difference vs K Value')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Average Skewness Difference')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best K seems to be 4, as it is the k for which we locate an \"elbow\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rows with missing values:\n",
    "nans_index = data[mv_metric_features].isna().any(axis=1)\n",
    "data_mv[nans_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "data_mv[mv_metric_features] = imputer.fit_transform(data_mv[mv_metric_features]).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at an example row, rows without any activities have sum = 0. \\\n",
    "If any of these rows have missing values in an activity, they will be filled with 1 so each client has at least 1 activity done in the sports facility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mv[activities].loc[24824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mv[activities].loc[24824].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in data_mv.index[nans_index].tolist():\n",
    "    if (data_mv.loc[idx, activities].sum() == 0) :\n",
    "        data_mv.loc[idx, activities] = data_mv[activities].loc[idx].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_categorical_features = ['AthleticsActivities','WaterActivities','FitnessActivities','TeamActivities','RacketActivities','CombatActivities','SpecialActivities','OtherActivities','HasReferences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Metric Features- Central Tendency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central tendency measures could also be applied to non-metric features, with the draw back that these methods possibly change distribution of features and can bring bias with it. Since our data is skewed, measured like median and mean can afect the distribution of our variables.\n",
    "\n",
    "Since the number of missing values is small for each categorical feature (~0.3%), this approach is also correct and used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = data_mv[mv_categorical_features].mode().loc[0]\n",
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mv.fillna(modes, inplace=True) #replace non-metric features with mode\n",
    "data_mv.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We compare the skewness of the data before and after imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_skewness(data, variables, name):\n",
    "    aux = []\n",
    "    for feature in variables:\n",
    "        aux.append(\n",
    "            {\n",
    "                'Feature': feature,\n",
    "                'Skewness ' + name: data[feature].skew(),\n",
    "            }\n",
    "        )\n",
    "    output = pd.DataFrame(aux)\n",
    "    output.set_index('Feature', inplace = True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_skw = calculate_skewness(data, mv_categorical_features, \"data\")\n",
    "data_mv_skw = calculate_skewness(data_mv, mv_categorical_features, \"data_mv\")\n",
    "pd.concat([data_skw, data_mv_skw], join=\"outer\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_skw = calculate_skewness(data, mv_metric_features, \"data\")\n",
    "data_mv_skw = calculate_skewness(data_mv, mv_metric_features, \"mv\")\n",
    "pd.concat([data_skw, data_mv_skw], join=\"outer\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking difference in distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diferenciar Gráficos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 11))\n",
    "\n",
    "for ax, feat in zip(axes[0].flatten(), mv_metric_features): \n",
    "    ax.hist(data[feat], bins = 40)\n",
    "    ax.set_title(feat, y=-0.13)\n",
    "\n",
    "for ax, feat in zip(axes[1].flatten(), mv_metric_features): \n",
    "    ax.hist(data_mv[feat], bins = 40)\n",
    "    ax.set_title(feat, y=-0.13)\n",
    "    \n",
    "title = \" Numeric Variables' Histograms\"\n",
    "\n",
    "plt.suptitle(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig, axes = plt.subplots(2, ceil(len(mv_categorical_features) / 2), figsize=(20, 11))\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), mv_categorical_features): \n",
    "    sns.countplot(x=data_mv[feat].astype(object), ax=ax, color='#007acc')\n",
    "\n",
    "title = \"Categorical Variables' Absolute Frequencies\"\n",
    "plt.suptitle(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_mv.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fixing Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Since we are talking about a number of visits, we transform `AllowedNumberOfVisitsBySLA` into an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AllowedNumberOfVisitsBySLA'] = np.round(data['AllowedNumberOfVisitsBySLA']).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) We decide to keep `DaysWithoutFrequency` as a variable thta applies to all clients.\n",
    "Since we changed some values of `EnrollmentFinish`, we need to recalculate this feature so all values are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DaysWithoutFrequency'] = (data['EnrollmentFinish'] -data['DateLastVisit']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We decide to turn our datetime features into the variables `Active_Period` (number of days of last period of activity) and `Contract_Duration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Active_Period'] = (data['LastPeriodFinish']- data['LastPeriodStart'])\n",
    "data['Active_Period']= data['Active_Period'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Contract_Duration'] = (data['EnrollmentFinish']- data['EnrollmentStart'])\n",
    "data['Contract_Duration']=data['Contract_Duration'].dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) We need to check entries where `LifetimeValue` is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have only three entries, these are droped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.index[data['LifetimeValue'] == 0].tolist(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to create new features based on the features we were initially given.\\\n",
    "Our goal is to create as many new features as possible with the information we were given. These features are then selected based on their relevancy and redudancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Real number of visits in relation to the allowed number of visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PercentageOfVisits'] = ((data['RealNumberOfVisits'] / data['AllowedNumberOfVisitsBySLA'])).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Total number of Activities the client is signed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalNumberOfActivities'] = data.iloc[:, 11:19].sum(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clients can't have no activity and `UseByTime` being equal to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['TotalNumberOfActivities']==0]['UseByTime'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['TotalNumberOfActivities']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.index[data['TotalNumberOfActivities'] == 0].tolist(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Monthly paid value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is considered that a sport facility has monthly payments, it is important to understand how much a client pays each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalMonths'] = (data['EnrollmentFinish'] - data['EnrollmentStart']) // np.timedelta64(1, 'M')\n",
    "data['TotalMonths'] = np.where(data['TotalMonths'] <= 0, 1, data['TotalMonths']) #cases in which is less than one month, we will assume one month\n",
    "\n",
    "data['MonthlyValue'] = (data['LifetimeValue'] / data['TotalMonths']).round(2)\n",
    "\n",
    "data.drop('TotalMonths', axis=1, inplace=True) #drop total months column since we only needed it for this code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Percentage of visits that were classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PercentageOfClasses'] = (data['AttendedClasses'] / data['NumberOfFrequencies'] * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Number of visits the client made to the facility during their contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Freq_Visits_Day']= (data['NumberOfFrequencies'] / data['Contract_Duration']).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Frequency of classes attended during contract duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Freq_Classes_Contract']= (data['AttendedClasses'] / data['Contract_Duration']).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Frequency of visits made during active period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Visits_ActivePeriod'] = (data['RealNumberOfVisits'] / data['Active_Period']).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop datetime features as they have no importance for clustering purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['EnrollmentStart','EnrollmentFinish','LastPeriodStart','LastPeriodFinish','DateLastVisit'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis done previously, only the variable 'Gender' is not encoded. \\\n",
    "We chose to encode this variable into a bianry variable where 1= 'Female' and 0 = 'Male'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_data = pd.get_dummies(data, columns=['Gender'], dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_data.drop('Gender_Male', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ohe_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Normalization/Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have created new features, we re-define our lists of metric and non-metric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_metric_features = ['UseByTime','AthleticsActivities', 'WaterActivities', 'FitnessActivities','TeamActivities', 'RacketActivities', 'CombatActivities','SpecialActivities', 'OtherActivities','HasReferences','Dropout','Gender_Female']\n",
    "metric_features = data.columns.drop(non_metric_features ).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a first approach, using a *MinMaxScaler* is considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_minmax = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_feat = scaler.fit_transform(data_minmax[metric_features])\n",
    "scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_minmax[metric_features] = scaled_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with skewed data with the presence of outliers outliers, using a `RobustScaler` is considered ideal as this scaler is not sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_robust = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "scaled_feat = scaler.fit_transform(data_robust[metric_features])\n",
    "scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_robust[metric_features] = scaled_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare both scaling methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 11))\n",
    "\n",
    "for ax, feat in zip(axes[0].flatten(), metric_features): \n",
    "    ax.hist(data_minmax[feat], bins = 40)\n",
    "    ax.set_title(feat, y=-0.13)\n",
    "\n",
    "for ax, feat in zip(axes[1].flatten(), metric_features): \n",
    "    ax.hist(data_robust[feat], bins = 40)\n",
    "    ax.set_title(feat, y=-0.13)\n",
    "    \n",
    "\n",
    "title = \"MinMax VS  Robust Numeric Variables' Histograms\"\n",
    "\n",
    "plt.suptitle(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers cause the mean and standard deviation to soar to much higher values. The standard scaler uses these inflated values. Thus, it reduces the relative distance between outliers and other data points.\n",
    "\n",
    "Hence when outliers are present, the standard scaler produces a distorted view of the original distribution.\n",
    "\n",
    "Robust scaler doesn’t suffer from this defect. It resists the pull of outliers. Its scaled values have enough range so that the distance between outliers and other values remains largely intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_robust.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection: Redundancy VS Relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High correlation between variables will bias your results and give too much importance to these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare figure\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Obtain correlation matrix. Round the values to 2 decimal cases. Use the DataFrame corr() and round() method.\n",
    "corr = np.round(data[metric_features].corr(method=\"pearson\"), decimals=2)\n",
    "\n",
    "# Build annotation matrix (values above |0.5| will appear annotated in the plot)\n",
    "mask_annot = np.absolute(corr.values)>= 0.5\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\")) # Try to understand what this np.where() does\n",
    "\n",
    "# Plot heatmap of the correlation matrix\n",
    "sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True), \n",
    "            fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
    "\n",
    "# Layout\n",
    "fig.subplots_adjust(top=0.95)\n",
    "fig.suptitle(\"Correlation Matrix\", fontsize=20)\n",
    "\n",
    "#plt.savefig(os.path.join('..', 'figures', 'exp_analysis', 'correlation_matrix.png'), dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uncover some important information:\n",
    "- `Age` and  `Income` are highly correlated.\n",
    "- `LifetimeValue` is also very correlated with `NumberOfRenewals`, `NumberOfFrequencies`, `AttendedClasses` and `ContractDuration`;\n",
    "- `NumberOfFrequencies` is correlated with `NumberOfRenewals` and `ContractDuration`\n",
    "- `AttendedClasses` is correlated with `PercentageOfClasses` and `Freq_Classes_Contract`\n",
    "- `AllowedWeeklyVisitsBySLA` is highly negatively correlated with `PercentageOfClasses` and correlated with `AllowedNumberOfVisitsBySLA`\n",
    "- `Visits_ActivePeriord` is highly correlated with `RealNumberOfVisits`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We remove `Income` as we are looking to understand the different age groups in our sports facility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"Income\", axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) We remove `RealNumberOfVisits` as `Visits_ActivePeriod` is more relevant to study client behaviour as it is a value that is defined in the same time frame for all clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"RealNumberOfVisits\", axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We drop `DaysWithoutFrequency` as it has no correlation with other variables and can be considered \"noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"DaysWithoutFrequency\", axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) For the same reasons we drop `TotalNumberOfActivities`, `MonthlyValue`, `ActivePeriod` and `NumberOfReferences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"TotalNumberOfActivities\",\"MonthlyValue\", \"Active_Period\", \"NumberOfReferences\" , \"HasReferences\"], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we drop `NumberOfReferences` and `HasReferences` is related with this variable and highly unbalanced, we decide to drop it too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We drop `LifetimeValue` as it has correlations with many other variable and can be considered redunctant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"LifetimeValue\", axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Between `AttendedClasses` , `Freq_Classes_Contract` and `PercentageOfClasses`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"AttendedClasses\", axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) `AllowedWeeklyVisitsBySLA` and `AllowedNumberOfVisitsBySLA` and ``RealNumberOfVisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"AllowedWeeklyVisitsBySLA\", axis =1, inplace = True)\n",
    "data.drop(\"AllowedNumberOfVisitsBySLA\", axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Between `NumberOfRenewals` and `ContractDuration`, the second variable is kept as we have no information about the renewal process of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"NumberOfRenewals\", axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) `NumberOfFrequencies` is droped once we have other variables such as `Freq_Visits_Day` that offers a more informative insight about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"NumberOfFrequencies\", axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we have percentage of classes and freq, we keep both as we consider they offer differentet info.\n",
    "Same thing for visits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do:\n",
    "- General\n",
    "- Activities acording to age (maybe others), gender\n",
    "- Attendance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering by Perspectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features = ['Age','Contract_Duration', 'PercentageOfVisits', 'PercentageOfClasses',\n",
    "       'Freq_Visits_Day', 'Freq_Classes_Contract', 'Visits_ActivePeriod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection of the reamining outliers trough DBSCAN\n",
    "# epsilon of 2\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors = (2 * len(metric_features)) - 1)\n",
    "neigh.fit(data[metric_features])\n",
    "distances, _ = neigh.kneighbors(data[metric_features])\n",
    "distances = np.sort(distances[:, -1])\n",
    "#plt.yticks(np.arange(1,22,1))\n",
    "plt.plot(distances, color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=1.8, min_samples = 2 * len(metric_features), n_jobs = -1)\n",
    "dbscan_labels = dbscan.fit_predict(data[metric_features])\n",
    "\n",
    "Counter(dbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, pd.Series(dbscan_labels, name = 'dbscan_labels', index = data.index)], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame after preprocessing\n",
    "# Replace 'df' with the name of your DataFrame variable\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_filename = \"processed_dataset.csv\"\n",
    "data.to_csv(output_filename, index=False)  # Set 'index=False' to exclude row indices from the file\n",
    "\n",
    "print(f\"Dataset saved as {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dbscan_out = data[dbscan_labels == -1]\n",
    "data = data[dbscan_labels != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K means + hierarchial clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance_perspective = [ 'Contract_Duration', 'PercentageOfVisits', 'PercentageOfClasses',\n",
    "       'Freq_Visits_Day', 'Freq_Classes_Contract', 'Visits_ActivePeriod']\n",
    "compare = ['UseByTime','Dropout', 'Gender_Female', 'Age']\n",
    "df_activities = data[activities].copy()\n",
    "df_attendance = data[attendance_perspective].copy()\n",
    "df_compare = data[compare].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import clone\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def get_ss(df):\n",
    "    \"\"\"Computes the sum of squares for all variables given a dataset\n",
    "    \"\"\"\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable\n",
    "\n",
    "def r2(df, labels):\n",
    "    sst = get_ss(df)\n",
    "    ssw = np.sum(df.groupby(labels).apply(get_ss))\n",
    "    return 1 - ssw/sst\n",
    "    \n",
    "def get_r2_scores(df, clusterer, min_k=2, max_k=10):\n",
    "    \"\"\"\n",
    "    Loop over different values of k. To be used with sklearn clusterers.\n",
    "    \"\"\"\n",
    "    r2_clust = {}\n",
    "    for n in range(min_k, max_k):\n",
    "        clust = clone(clusterer).set_params(n_clusters=n)\n",
    "        labels = clust.fit_predict(df)\n",
    "        r2_clust[n] = r2(df, labels)\n",
    "    return r2_clust\n",
    "\n",
    "\n",
    "# Set up the clusterers\n",
    "kmeans = KMeans(\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hierarchical = AgglomerativeClustering(\n",
    "    metric='euclidean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the optimal cluster on demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on demographic variables\n",
    "r2_scores = {}\n",
    "r2_scores['kmeans'] = get_r2_scores(df_attendance, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores[linkage] = get_r2_scores(\n",
    "        df_attendance, hierarchical.set_params(linkage=linkage)\n",
    "    )\n",
    "\n",
    "pd.DataFrame(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_att = KMeans(n_clusters=35, init = 'k-means++', n_init=20, random_state=93)\n",
    "km_att_labels = kmeans_att.fit_predict(df_attendance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_km_att = pd.concat([df_attendance, pd.Series(km_att_labels, name='km_att_labels', index=df_attendance.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame(kmeans_att.cluster_centers_, columns = df_attendance.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hclust = AgglomerativeClustering(linkage='ward', affinity='euclidean', distance_threshold=0, n_clusters=None)\n",
    "hclust_labels = hclust.fit_predict(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_km_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            current_count += 1\n",
    "        else:\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "y_threshold = 3\n",
    "dendrogram(linkage_matrix, truncate_mode='level', labels=centroids.index, p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'Euclidean Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-running the Hierarchical clustering based on the correct number of clusters\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    affinity='euclidean', \n",
    "    n_clusters=4\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(centroids)\n",
    "centroids['hclust_labels'] = hclust_labels\n",
    "\n",
    "centroids  # centroid's cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper between concatenated hierarchical clusters\n",
    "cluster_mapper = centroids['hclust_labels'].to_dict()\n",
    "\n",
    "# Mapping the hierarchical clusters on the centroids to the observations\n",
    "df_km_att['kmeans_labels'] = df_km_att.apply(\n",
    "    lambda row: cluster_mapper[(row['km_att_labels'])], axis=1)\n",
    "\n",
    "df_km_att.drop('km_att_labels', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_km_att.groupby('kmeans_labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, compar_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    if compar_titles == None:\n",
    "        compar_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2, figsize=figsize, squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        \n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "       \n",
    "        pd.plotting.parallel_coordinates(centroids, label, color=sns.color_palette(), ax=ax[0])\n",
    "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1])\n",
    "\n",
    "        \n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].legend(handles, cluster_labels) \n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=-20)\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profiles(df_km_att, ['kmeans_labels'], (20,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster dois bue pouca frequencia, normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activities.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"kmeans_labels\"] = df_km_att['kmeans_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A age nao pode estar obviamente aqui lol que burra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare_1 = data[['kmeans_labels',\n",
    "              'UseByTime', 'Dropout', 'Gender_Female', 'Age']].groupby(['kmeans_labels']).sum()\n",
    "\n",
    "df_compare_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare_2 = data[['kmeans_labels',\n",
    "              'AthleticsActivities', 'WaterActivities', 'FitnessActivities',\n",
    "                'TeamActivities', 'RacketActivities', 'CombatActivities',\n",
    "                'SpecialActivities', 'OtherActivities']].groupby(['kmeans_labels']).sum()\n",
    "\n",
    "df_compare_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "df_compare_1.plot(kind='bar', stacked=False, ax=ax)\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "df_compare_2.plot(kind='bar', stacked=False, ax=ax)\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the R² scores for each cluster solution on demographic variables\n",
    "pd.DataFrame(r2_scores).plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Attendance Variables:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R² metric\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_att = KMeans(\n",
    "    n_clusters=4,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_labels = kmeans_att.fit_predict(df_attendance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"attendance_labels\"] = att_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data[\"attendance_labels\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features = ['Contract_Duration',\n",
    " 'PercentageOfVisits',\n",
    " 'PercentageOfClasses',\n",
    " 'Freq_Visits_Day',\n",
    " 'Freq_Classes_Contract',\n",
    " 'Visits_ActivePeriod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centroids = data.groupby(['attendance_labels'])[metric_features].mean()\n",
    "\n",
    "linkage = 'ward'\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage=linkage, \n",
    "    metric='euclidean', \n",
    "    distance_threshold=0, \n",
    "    n_clusters=None\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame(kmeans_att.cluster_centers_, columns = df_attendance.columns)\n",
    "hclust = AgglomerativeClustering(linkage='ward', affinity='euclidean', distance_threshold=0, n_clusters=None)\n",
    "\n",
    "hclust_labels = hclust.fit_predict(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            current_count += 1\n",
    "        else:\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "y_threshold = 6\n",
    "dendrogram(linkage_matrix, truncate_mode='level', labels=centroids.index, p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'Euclidean Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing HC\n",
    "hclust = AgglomerativeClustering(linkage='ward', metric='euclidean', n_clusters=5)\n",
    "hc_labels = hclust.fit_predict(data[metric_features])\n",
    "hc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the clusters\n",
    "df_concat = pd.concat((data, pd.Series(hc_labels, name='labels', index=data.index)), axis=1)\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOM + K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This som implementation does not have a random seed parameter\n",
    "# We're going to set it up ourselves\n",
    "np.random.seed(42)\n",
    "\n",
    "# Notice that the SOM did not converge - We're under a time constraint for this class\n",
    "sm = sompy.SOMFactory().build(\n",
    "    data[metric_features].values, \n",
    "    mapsize=[50, 50],  # NEEDS TO BE A LIST\n",
    "    initialization='random',\n",
    "    neighborhood='gaussian',\n",
    "    training='batch',\n",
    "    lattice='hexa',\n",
    "    component_names=metric_features\n",
    ")\n",
    "\n",
    "## This will take a few minutes!\n",
    "# sm.train(n_job=-1, verbose='info', train_rough_len=100, train_finetune_len=100)\n",
    "sm.train(n_job=-1, verbose='info', train_rough_len=50, train_finetune_len=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_clusters = range(1, 11)\n",
    "\n",
    "inertia = []\n",
    "for n_clus in range_clusters:  # iterate over desired ncluster range\n",
    "    kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=15, random_state=1)\n",
    "    kmclust.fit(data[metric_features])\n",
    "    inertia.append(kmclust.inertia_)  # save the inertia of the given cluster solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inertia plot\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(range_clusters, inertia)\n",
    "plt.ylabel(\"Inertia: SSw\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"Inertia plot over clusters\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on top of the 2500 units (sm.get_node_vectors() output)\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', n_init=20, random_state=42)\n",
    "nodeclus_labels = kmeans.fit_predict(sm.codebook.matrix)\n",
    "sm.cluster_labels = nodeclus_labels  # setting the cluster labels of sompy\n",
    "\n",
    "hits = HitMapView(12, 12,\"Clustering\", text_size=10)\n",
    "hits.show(sm, anotate=True, onlyzeros=False, labelsize=7, cmap=\"Pastel1\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile Report: Might help us to get some info (delete later but refer if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(\n",
    "    data, \n",
    "    title='Sports Facility Customer Data',\n",
    "    correlations={\n",
    "        \"pearson\": {\"calculate\": True},\n",
    "        \"spearman\": {\"calculate\": False},\n",
    "        \"kendall\": {\"calculate\": False},\n",
    "        \"phi_k\": {\"calculate\": False},\n",
    "        \"cramers\": {\"calculate\": False},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2324",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
